{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WNK7vbHo-KYU"
   },
   "source": [
    "## Bayesian methods of hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BlFdvPwF-KYW"
   },
   "source": [
    "In addition to the random search and the grid search methods for selecting optimal hyperparameters, we can use Bayesian methods of probabilities to select the optimal hyperparameters for an algorithm.\n",
    "\n",
    "In this case study, we will be using the BayesianOptimization library to perform hyperparmater tuning. This library has very good documentation which you can find here: https://github.com/fmfn/BayesianOptimization\n",
    "\n",
    "You will need to install the Bayesian optimization module. Running a cell with an exclamation point in the beginning of the command will run it as a shell command — please do this to install this module from our notebook in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pssx080d-Ulf"
   },
   "outputs": [],
   "source": [
    "#! pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T16:39:09.312682Z",
     "start_time": "2019-04-22T16:39:09.309208Z"
    },
    "_kg_hide-input": true,
    "colab": {},
    "colab_type": "code",
    "id": "l9nfFTyj-KYY"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm\n",
    "from bayes_opt import BayesianOptimization\n",
    "from catboost import CatBoostClassifier, cv, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "D16Dquw1AAK0",
    "outputId": "44167587-f22e-4bf5-a816-e2bcfdc6c4ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " '.ipynb_checkpoints',\n",
       " 'Bayesian_optimization_case_study.ipynb',\n",
       " 'flight_delays_test.csv.zip',\n",
       " 'flight_delays_train.csv.zip']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T14:48:15.929012Z",
     "start_time": "2019-04-22T14:48:15.926574Z"
    },
    "colab_type": "text",
    "id": "AkBt3yds-KYu"
   },
   "source": [
    "## How does Bayesian optimization work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E1kyBCUs-KYv"
   },
   "source": [
    "Bayesian optimization works by constructing a posterior distribution of functions (Gaussian process) that best describes the function you want to optimize. As the number of observations grows, the posterior distribution improves, and the algorithm becomes more certain of which regions in parameter space are worth exploring and which are not, as seen in the picture below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gAdHF72R-KYw"
   },
   "source": [
    "<img src=\"https://github.com/fmfn/BayesianOptimization/blob/master/examples/bo_example.png?raw=true\" />\n",
    "As you iterate over and over, the algorithm balances its needs of exploration and exploitation while taking into account what it knows about the target function. At each step, a Gaussian Process is fitted to the known samples (points previously explored), and the posterior distribution, combined with an exploration strategy (such as UCB — aka Upper Confidence Bound), or EI (Expected Improvement). This process is used to determine the next point that should be explored (see the gif below).\n",
    "<img src=\"https://github.com/fmfn/BayesianOptimization/raw/master/examples/bayesian_optimization.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RTP8KUlLoYzu"
   },
   "source": [
    "## Let's look at a simple example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "crpPqKdC-KYx"
   },
   "source": [
    "The first step is to create an optimizer. It uses two items:\n",
    "* function to optimize\n",
    "* bounds of parameters\n",
    "\n",
    "The function is the procedure that counts metrics of our model quality. The important thing is that our optimization will maximize the value on function. Smaller metrics are best. Hint: don't forget to use negative metric values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e09ciF8gpTfr"
   },
   "source": [
    "Here we define our simple function we want to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ofwvnfEwo5mG"
   },
   "outputs": [],
   "source": [
    "def simple_func(a, b):\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XCGsdciCpeI3"
   },
   "source": [
    "Now, we define our bounds of the parameters to optimize, within the Bayesian optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4jLYW2qnpOFr"
   },
   "outputs": [],
   "source": [
    "optimizer = BayesianOptimization(\n",
    "    simple_func,\n",
    "    {'a': (1, 3),\n",
    "    'b': (4, 7)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dg6LdYx8pq2T"
   },
   "source": [
    "These are the main parameters of this function:\n",
    "\n",
    "* **n_iter:** This is how many steps of Bayesian optimization you want to perform. The more steps, the more likely you are to find a good maximum.\n",
    "\n",
    "* **init_points:** This is how many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i-GKMJ1uqMYv"
   },
   "source": [
    "Let's run an example where we use the optimizer to find the best values to maximize the target value for a and b given the inputs of 3 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "Oy44Ro7wqNat",
    "outputId": "9cc64d54-b1e6-46d1-dc29-4c0039a1c72d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |     a     |     b     |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m8.4      \u001b[0m | \u001b[0m1.925    \u001b[0m | \u001b[0m6.475    \u001b[0m |\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m7.687    \u001b[0m | \u001b[0m1.907    \u001b[0m | \u001b[0m5.78     \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m6.736    \u001b[0m | \u001b[0m2.599    \u001b[0m | \u001b[0m4.137    \u001b[0m |\n",
      "| \u001b[95m4        \u001b[0m | \u001b[95m8.473    \u001b[0m | \u001b[95m1.473    \u001b[0m | \u001b[95m7.0      \u001b[0m |\n",
      "| \u001b[95m5        \u001b[0m | \u001b[95m10.0     \u001b[0m | \u001b[95m3.0      \u001b[0m | \u001b[95m7.0      \u001b[0m |\n",
      "=================================================\n"
     ]
    }
   ],
   "source": [
    "optimizer.maximize(3,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tyKFMF2Hq2Sx"
   },
   "source": [
    "Great, now let's print the best parameters and the associated maximized target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "_H6DixyfscV_",
    "outputId": "fd0c35d7-e30d-4d30-9ab2-12c0fa837971"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 3.0, 'b': 7.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(optimizer.max['params']);optimizer.max['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tQ1T1V6Mspi4"
   },
   "source": [
    "## Test it on real data using the Light GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y_oGwREZkm4h"
   },
   "source": [
    "The dataset we will be working with is the famous flight departures dataset. Our modeling goal will be to predict if a flight departure is going to be delayed by 15 minutes based on the other attributes in our dataset. As part of this modeling exercise, we will use Bayesian hyperparameter optimization to identify the best parameters for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "abYSagjQANDZ"
   },
   "source": [
    "**<font color='teal'> You can load the zipped csv files just as you would regular csv files using Pandas read_csv. In the next cell load the train and test data into two seperate dataframes. </font>**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EWKBApVuAeJe"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('flight_delays_train.csv.zip')\n",
    "test_df = pd.read_csv('flight_delays_test.csv.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OapNcT9Eikis"
   },
   "source": [
    "**<font color='teal'> Print the top five rows of the train dataframe and review the columns in the data. </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "__4cXZ8iiYaC",
    "outputId": "8718ad4b-8955-486c-9ae8-1dee6aa6c2fb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>DayofMonth</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>DepTime</th>\n",
       "      <th>UniqueCarrier</th>\n",
       "      <th>Origin</th>\n",
       "      <th>Dest</th>\n",
       "      <th>Distance</th>\n",
       "      <th>dep_delayed_15min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c-8</td>\n",
       "      <td>c-21</td>\n",
       "      <td>c-7</td>\n",
       "      <td>1934</td>\n",
       "      <td>AA</td>\n",
       "      <td>ATL</td>\n",
       "      <td>DFW</td>\n",
       "      <td>732</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c-4</td>\n",
       "      <td>c-20</td>\n",
       "      <td>c-3</td>\n",
       "      <td>1548</td>\n",
       "      <td>US</td>\n",
       "      <td>PIT</td>\n",
       "      <td>MCO</td>\n",
       "      <td>834</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c-9</td>\n",
       "      <td>c-2</td>\n",
       "      <td>c-5</td>\n",
       "      <td>1422</td>\n",
       "      <td>XE</td>\n",
       "      <td>RDU</td>\n",
       "      <td>CLE</td>\n",
       "      <td>416</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c-11</td>\n",
       "      <td>c-25</td>\n",
       "      <td>c-6</td>\n",
       "      <td>1015</td>\n",
       "      <td>OO</td>\n",
       "      <td>DEN</td>\n",
       "      <td>MEM</td>\n",
       "      <td>872</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c-10</td>\n",
       "      <td>c-7</td>\n",
       "      <td>c-6</td>\n",
       "      <td>1828</td>\n",
       "      <td>WN</td>\n",
       "      <td>MDW</td>\n",
       "      <td>OMA</td>\n",
       "      <td>423</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Month DayofMonth DayOfWeek  DepTime UniqueCarrier Origin Dest  Distance  \\\n",
       "0   c-8       c-21       c-7     1934            AA    ATL  DFW       732   \n",
       "1   c-4       c-20       c-3     1548            US    PIT  MCO       834   \n",
       "2   c-9        c-2       c-5     1422            XE    RDU  CLE       416   \n",
       "3  c-11       c-25       c-6     1015            OO    DEN  MEM       872   \n",
       "4  c-10        c-7       c-6     1828            WN    MDW  OMA       423   \n",
       "\n",
       "  dep_delayed_15min  \n",
       "0                 N  \n",
       "1                 N  \n",
       "2                 N  \n",
       "3                 N  \n",
       "4                 Y  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UxGBsPQhffgd"
   },
   "source": [
    "**<font color='teal'> Use the describe function to review the numeric columns in the train dataframe. </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "_bRRKG3DAtae",
    "outputId": "7cfb9975-ec97-422c-abbd-98923a0b7aec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DepTime</th>\n",
       "      <th>Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1341.523880</td>\n",
       "      <td>729.39716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>476.378445</td>\n",
       "      <td>574.61686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>30.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>931.000000</td>\n",
       "      <td>317.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1330.000000</td>\n",
       "      <td>575.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1733.000000</td>\n",
       "      <td>957.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2534.000000</td>\n",
       "      <td>4962.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             DepTime      Distance\n",
       "count  100000.000000  100000.00000\n",
       "mean     1341.523880     729.39716\n",
       "std       476.378445     574.61686\n",
       "min         1.000000      30.00000\n",
       "25%       931.000000     317.00000\n",
       "50%      1330.000000     575.00000\n",
       "75%      1733.000000     957.00000\n",
       "max      2534.000000    4962.00000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i6k-_fI5Aiyh"
   },
   "source": [
    "Notice, `DepTime` is the departure time in a numeric representation in 2400 hours. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gtZS4-hrlQah"
   },
   "source": [
    " **<font color='teal'>The response variable is 'dep_delayed_15min' which is a categorical column, so we need to map the Y for yes and N for no values to 1 and 0. Run the code in the next cell to do this.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:38:42.677690Z",
     "start_time": "2019-04-22T15:38:42.481963Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "yRlOTbnW-KYc"
   },
   "outputs": [],
   "source": [
    "#train_df = train_df[train_df.DepTime <= 2400].copy()\n",
    "y_train = train_df['dep_delayed_15min'].map({'Y': 1, 'N': 0}).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z3WPkFQO9uo9"
   },
   "source": [
    "## Feature Engineering\n",
    "Use these defined functions to create additional features for the model. Run the cell to add the functions to your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cXqsqz5W9t3r"
   },
   "outputs": [],
   "source": [
    "def label_enc(df_column):\n",
    "    df_column = LabelEncoder().fit_transform(df_column)\n",
    "    return df_column\n",
    "\n",
    "def make_harmonic_features_sin(value, period=2400):\n",
    "    value *= 2 * np.pi / period \n",
    "    return np.sin(value)\n",
    "\n",
    "def make_harmonic_features_cos(value, period=2400):\n",
    "    value *= 2 * np.pi / period \n",
    "    return np.cos(value)\n",
    "\n",
    "def feature_eng(df):\n",
    "    df['flight'] = df['Origin']+df['Dest']\n",
    "    df['Month'] = df.Month.map(lambda x: x.split('-')[-1]).astype('int32')\n",
    "    df['DayofMonth'] = df.DayofMonth.map(lambda x: x.split('-')[-1]).astype('uint8')\n",
    "    df['begin_of_month'] = (df['DayofMonth'] < 10).astype('uint8')\n",
    "    df['midddle_of_month'] = ((df['DayofMonth'] >= 10)&(df['DayofMonth'] < 20)).astype('uint8')\n",
    "    df['end_of_month'] = (df['DayofMonth'] >= 20).astype('uint8')\n",
    "    df['DayOfWeek'] = df.DayOfWeek.map(lambda x: x.split('-')[-1]).astype('uint8')\n",
    "    df['hour'] = df.DepTime.map(lambda x: x/100).astype('int32')\n",
    "    df['morning'] = df['hour'].map(lambda x: 1 if (x <= 11)& (x >= 7) else 0).astype('uint8')\n",
    "    df['day'] = df['hour'].map(lambda x: 1 if (x >= 12) & (x <= 18) else 0).astype('uint8')\n",
    "    df['evening'] = df['hour'].map(lambda x: 1 if (x >= 19) & (x <= 23) else 0).astype('uint8')\n",
    "    df['night'] = df['hour'].map(lambda x: 1 if (x >= 0) & (x <= 6) else 0).astype('int32')\n",
    "    df['winter'] = df['Month'].map(lambda x: x in [12, 1, 2]).astype('int32')\n",
    "    df['spring'] = df['Month'].map(lambda x: x in [3, 4, 5]).astype('int32')\n",
    "    df['summer'] = df['Month'].map(lambda x: x in [6, 7, 8]).astype('int32')\n",
    "    df['autumn'] = df['Month'].map(lambda x: x in [9, 10, 11]).astype('int32')\n",
    "    df['holiday'] = (df['DayOfWeek'] >= 5).astype(int) \n",
    "    df['weekday'] = (df['DayOfWeek'] < 5).astype(int)\n",
    "    df['airport_dest_per_month'] = df.groupby(['Dest', 'Month'])['Dest'].transform('count')\n",
    "    df['airport_origin_per_month'] = df.groupby(['Origin', 'Month'])['Origin'].transform('count')\n",
    "    df['airport_dest_count'] = df.groupby(['Dest'])['Dest'].transform('count')\n",
    "    df['airport_origin_count'] = df.groupby(['Origin'])['Origin'].transform('count')\n",
    "    df['carrier_count'] = df.groupby(['UniqueCarrier'])['Dest'].transform('count')\n",
    "    df['carrier_count_per month'] = df.groupby(['UniqueCarrier', 'Month'])['Dest'].transform('count')\n",
    "    df['deptime_cos'] = df['DepTime'].map(make_harmonic_features_cos)\n",
    "    df['deptime_sin'] = df['DepTime'].map(make_harmonic_features_sin)\n",
    "    df['flightUC'] = df['flight']+df['UniqueCarrier']\n",
    "    df['DestUC'] = df['Dest']+df['UniqueCarrier']\n",
    "    df['OriginUC'] = df['Origin']+df['UniqueCarrier']\n",
    "    return df.drop('DepTime', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-BYbxXpU-FGE"
   },
   "source": [
    "Concatenate the training and testing dataframes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cj6bfSNw_RAf"
   },
   "outputs": [],
   "source": [
    "full_df = pd.concat([train_df.drop('dep_delayed_15min', axis=1), test_df])\n",
    "full_df = feature_eng(full_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GSO8JbfM_W-F"
   },
   "source": [
    "Apply the earlier defined feature engineering functions to the full dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x6RfAINftjwi"
   },
   "outputs": [],
   "source": [
    "for column in ['UniqueCarrier', 'Origin', 'Dest','flight',  'flightUC', 'DestUC', 'OriginUC']:\n",
    "    full_df[column] = label_enc(full_df[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IJAw1RGB_ZuM"
   },
   "source": [
    "\n",
    "Split the new full dataframe into X_train and X_test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "15cPtQU5tjfz"
   },
   "outputs": [],
   "source": [
    "X_train = full_df[:train_df.shape[0]]\n",
    "X_test = full_df[train_df.shape[0]:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "umfAw-9JErLV"
   },
   "source": [
    "Create a list of the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T14:31:58.412296Z",
     "start_time": "2019-04-22T14:31:58.409088Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "5ibeVyNb-KZI"
   },
   "outputs": [],
   "source": [
    "categorical_features = ['Month',  'DayOfWeek', 'UniqueCarrier', 'Origin', 'Dest','flight',  'flightUC', 'DestUC', 'OriginUC']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NzMIsMPIETVk"
   },
   "source": [
    "Let's build a light GBM model to test the bayesian optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:18:04.466965Z",
     "start_time": "2019-04-22T15:18:04.457992Z"
    },
    "colab_type": "text",
    "id": "2hfm1i5G-KZH"
   },
   "source": [
    "### [LightGBM](https://lightgbm.readthedocs.io/en/latest/) is a gradient boosting framework that uses tree-based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n",
    "\n",
    "* Faster training speed and higher efficiency.\n",
    "* Lower memory usage.\n",
    "* Better accuracy.\n",
    "* Support of parallel and GPU learning.\n",
    "* Capable of handling large-scale data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jf-3F2Wg-KZL"
   },
   "source": [
    "First, we define the function we want to maximize and that will count cross-validation metrics of lightGBM for our parameters.\n",
    "\n",
    "Some params such as num_leaves, max_depth, min_child_samples, min_data_in_leaf should be integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:40:14.034265Z",
     "start_time": "2019-04-22T15:40:14.027868Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "LyUJBhGX-KZM"
   },
   "outputs": [],
   "source": [
    "def lgb_eval(num_leaves, max_depth, lambda_l2, lambda_l1, min_child_samples, min_data_in_leaf):\n",
    "    params = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"auc\",\n",
    "        'is_unbalance': True,\n",
    "        \"num_leaves\": int(num_leaves),\n",
    "        \"max_depth\": int(max_depth),\n",
    "        \"lambda_l2\": lambda_l2,\n",
    "        \"lambda_l1\": lambda_l1,\n",
    "        \"num_threads\": 20,\n",
    "        \"min_child_samples\": int(min_child_samples),\n",
    "        'min_data_in_leaf': int(min_data_in_leaf),\n",
    "        \"learning_rate\": 0.03,\n",
    "        \"subsample_freq\": 5,\n",
    "        \"bagging_seed\": 42,\n",
    "        \"verbosity\": -1\n",
    "    }\n",
    "    lgtrain = lightgbm.Dataset(X_train, y_train, categorical_feature=categorical_features)\n",
    "    cv_result = lightgbm.cv(params,\n",
    "                            lgtrain,\n",
    "                            num_boost_round=1000,\n",
    "                            stratified=True,\n",
    "                            nfold=3)\n",
    "    print(cv_result) \n",
    "    return cv_result  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FJwqBhdeF11Q"
   },
   "source": [
    "Apply the Bayesian optimizer to the function we created in the previous step to identify the best hyperparameters. We will run 10 iterations and set init_points = 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:48:04.682447Z",
     "start_time": "2019-04-22T15:40:14.641634Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "JheCOkUE-KZP",
    "outputId": "8f37ee51-885d-44e4-cdcd-ceb7abd58b61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | lambda_l1 | lambda_l2 | max_depth | min_ch... | min_da... | num_le... |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "{'valid auc-mean': [0.6945522689936562, 0.7028172092572581, 0.7069528580671302, 0.7099077970889924, 0.712187414129001, 0.7135146803876493, 0.7148096546380217, 0.7155853968730438, 0.7162229838749526, 0.7168885634477028, 0.7171156163831554, 0.7173460706456352, 0.7174857453289866, 0.7178012199256211, 0.7179893708409789, 0.7181449355807029, 0.7181090103598056, 0.7181104636448542, 0.718125449044894, 0.7182920148967318, 0.7184315673228078, 0.7185788515045686, 0.7186050113493428, 0.7187240772187383, 0.718773432881973, 0.7187060275355269, 0.7187287423997448, 0.718615868528464, 0.7185317684322957, 0.7184796252457217, 0.7184103344096804, 0.7183014380864668, 0.7183189315153702, 0.7183255490855047, 0.7182467278192881, 0.7181625117293969, 0.7180957217467583, 0.7180979857940543, 0.7180518548857933, 0.7180489169558957, 0.7179371831140607, 0.71796576783108, 0.7179592833345373, 0.7178465692233208, 0.7178287109389224, 0.7177928141913957, 0.7177683410027521, 0.7175668590431131, 0.7174371160127432, 0.7173852697008418, 0.7172210627034116, 0.7171433296687049, 0.7170546554550689, 0.7170195933486094, 0.7168707873407382, 0.7167458017834099, 0.7167464416700661, 0.7166698449269285, 0.7166573722377332, 0.7165475537202185, 0.7164560266480838, 0.7163501293590513, 0.7162251473083864, 0.716095745369775, 0.7159445173606942, 0.7158800870322078, 0.7158178933553673, 0.715753844688885, 0.7157039612406386, 0.7156203299317855, 0.7155728574037615, 0.7155397973920206, 0.7154803378094381, 0.7153913869533248, 0.7152945769924836, 0.7151653984549644, 0.7150786552686595, 0.715019587346806, 0.7150109126090719, 0.7149766857170073, 0.7149069338921933, 0.7148026996126576, 0.7146826812917575, 0.7145757450706589, 0.7144475415232371, 0.7143078219892264, 0.7142252732632645, 0.7141414109702705, 0.7140117953618449, 0.7138635511812238, 0.7137598404354177, 0.7136197530122382, 0.7135534229639794, 0.7134064344725967, 0.7133237623131047, 0.7132541450467215, 0.7131831194976993, 0.7130965080783249, 0.7130153978326447, 0.7129555172192695, 0.7128595749068821, 0.7127367910720738, 0.7126700992716497, 0.7125688341953552, 0.7125009037611533, 0.7123693835729764, 0.7122597241822777, 0.7122216152603822, 0.7121335585907621, 0.7119915104866962, 0.7118675387578307, 0.711793591645672, 0.7117669057452418, 0.7117302779900144, 0.7116318132428239, 0.7115864853279764, 0.7115601770431615, 0.7114898997987886, 0.7114502949143277, 0.7114168901000998, 0.7114404266346384, 0.7113602625186477, 0.7112591641611231, 0.7112212840575456, 0.7111903128501696, 0.7111184260940632, 0.7110680621351438, 0.7109770733660284, 0.7109152606013449, 0.7108536625446087, 0.7107479080491969, 0.7106773725276773, 0.7106406462148073, 0.7105626728531614, 0.7105525249993536, 0.710452396473206, 0.7103814059457515, 0.7102842933899433, 0.710264673532781, 0.7101163867054328, 0.7100428372281153, 0.7100092920197185, 0.7099674783203085, 0.709940657005026, 0.7098527799821984, 0.7098709998601974, 0.7098063769077184, 0.7097369631305694, 0.7097275349763361, 0.7096618109067955, 0.7095903943035052, 0.7094919566609029, 0.7094447611354974, 0.7093776343599734, 0.7093694135246403, 0.7094108261360491, 0.709399431239999, 0.7093688820665767, 0.7092916795912568, 0.709296927268681, 0.7092772034943363, 0.7092706707155708, 0.7092515015884368, 0.7092190810591168, 0.7091590807611564, 0.7090284272706953, 0.7090127136596833, 0.7089970953075366, 0.7089396550240351, 0.7088881484184898, 0.7087829137923723, 0.7087185790455051, 0.7087045349295616, 0.7086971149515272, 0.7086735729039318, 0.7086343474302041, 0.7085923018661475, 0.7086182040754968, 0.708617662930718, 0.708583866027082, 0.7085781884418455, 0.7085743624093152, 0.7084900536868607, 0.7084509179794202, 0.7084170852375262, 0.7084051199617628, 0.7083117870059663, 0.7082199130819035, 0.708218155709694, 0.7081947230889488, 0.7081792155300906, 0.7081398377908354, 0.7081257993394469, 0.7081032951831167, 0.7080045110347748, 0.7079949741729643, 0.7079325707511245, 0.7079544676764442, 0.7079385590067991, 0.7078837422608525, 0.7078143811810008, 0.7077692740481542, 0.7077535245187728, 0.7077403583618014, 0.7077012703145834, 0.7076378206352691, 0.7076159893571902, 0.7075792144675382, 0.7075406068182891, 0.7075590419673233, 0.7075447200196686, 0.7075295723853371, 0.7075339266265473, 0.7075679480820405, 0.7075964700258468, 0.7075575424072387, 0.7075003142238417, 0.7075176844055769, 0.7074953803228897, 0.7074239813611142, 0.7073524827485453, 0.7072966837217766, 0.7072365845609422, 0.707226604440674, 0.7072175965142434, 0.7071979619966737, 0.7071637931522163, 0.7070961473027442, 0.7070763140087472, 0.7070318152636966, 0.7070016749200331, 0.7069898070502821, 0.7069519216464375, 0.7069376853929388, 0.706898724617393, 0.7069167056438098, 0.7068926865676163, 0.7068663314291742, 0.7068789876236387, 0.7068403525366641, 0.7068073544403085, 0.706821041359122, 0.7067889721382667, 0.7067868029625103, 0.7068171970194158, 0.7068211705407075, 0.7067557516636648, 0.706722165995275, 0.7066722625021322, 0.7066321820841516, 0.7065506883592123, 0.7065139189301949, 0.7064554756252092, 0.7064189326999286, 0.706350128533196, 0.7063541470875428, 0.7062956955260237, 0.7063223376405712, 0.7062989580987162, 0.7062621213965042, 0.7062047249035004, 0.7061626544730745, 0.706127318914069, 0.7060755338667821, 0.70603296370714, 0.7059920136988485, 0.7059609529740286, 0.7059762496840648, 0.7059817446513158, 0.7059243878988876, 0.7059413170435831, 0.7058896910262354, 0.7058538392880406, 0.7058580420664967, 0.7058098136051307, 0.7057822783753201, 0.7057783633505772, 0.7057535506395242, 0.7057443370992815, 0.7056678258646952, 0.705651496014634, 0.7056618364107369, 0.7056799180513765, 0.7056708053617976, 0.7056396660248673, 0.7056371927300781, 0.7056261757318004, 0.7056342239123241, 0.7056569462986237, 0.70561006457363, 0.7056577397785965, 0.7056351771118007, 0.7055823828223812, 0.7055347798767038, 0.7055612444654308, 0.7054647613054922, 0.7054052767694347, 0.7053833152781271, 0.7053709688194424, 0.7053182195008496, 0.7053060965196191, 0.705284302992235, 0.705262462428431, 0.7052197683846586, 0.7052220002900391, 0.7052443387764042, 0.7052376348128865, 0.7052390870941254, 0.7052570495020779, 0.7052651424665971, 0.705272309220784, 0.7052829741914682, 0.7052725331979436, 0.7052842030046992, 0.7052735927129725, 0.7052775587047192, 0.705289728392045, 0.7052432917804761, 0.7052331302542788, 0.705242113955737, 0.705204057409467, 0.705208585485383, 0.7052160418078706, 0.7051993946560701, 0.7051737494878716, 0.7051184481309268, 0.7050921518261367, 0.7050999428541035, 0.7051322519309275, 0.7051081659422215, 0.7050951110669232, 0.7050807267607823, 0.7050527981073569, 0.7050705203777605, 0.705114755451333, 0.7050616178671962, 0.7050564757859794, 0.7050715133000022, 0.7050720477345943, 0.705070335699013, 0.7050151387055966, 0.705000655202507, 0.7050129764327702, 0.705035604533729, 0.705006553478384, 0.7049939711067892, 0.7049242936362056, 0.7048870209454937, 0.704874829955283, 0.7048805894681364, 0.7049256421094331, 0.7049156103454314, 0.704905798169113, 0.7048563152720987, 0.7048628648624566, 0.704827790637382, 0.7047925106110876, 0.7047899849132726, 0.7047754335636611, 0.7047507147862803, 0.7047596187446206, 0.7047526445263833, 0.7047342213574458, 0.7047304055934666, 0.7047322309278582, 0.7047331980103504, 0.7046989424244464, 0.7046952084295918, 0.704685304972655, 0.7046570594326136, 0.7046682929064115, 0.7046284387653173, 0.704596919817555, 0.7046241738811837, 0.7046159871150932, 0.7046382338509147, 0.7046027760299829, 0.7045717942367825, 0.7045302704360702, 0.7045151445692851, 0.7045095544676263, 0.7044585568164519, 0.7044219441485807, 0.7044055732691653, 0.7044004588391842, 0.7044600580790683, 0.7044639480238594, 0.7044512492393443, 0.7044147799458482, 0.7044265290953806, 0.704453681538381, 0.7044332551875594, 0.7044339597791547, 0.7044501999263835, 0.7044476936438934, 0.7044622727647997, 0.7044627633382289, 0.7044758165563508, 0.7044493816114804, 0.7044240183985369, 0.7043877978476192, 0.7043695538191349, 0.7043533934970827, 0.7043464772759771, 0.7043472072672493, 0.7043535759950088, 0.7043234443806159, 0.7043146552176068, 0.7043079799680463, 0.7042928721647134, 0.7042918268224282, 0.7042755434870887, 0.7042291351652682, 0.7041973747791367, 0.7041627467847936, 0.7041664334033134, 0.7041535070997867, 0.7041439257842333, 0.7041366582307234, 0.7041533826545153, 0.7041100048455456, 0.7041453844209618, 0.7040957648294016, 0.7040750460276347, 0.7040758379826765, 0.704043422000388, 0.7040204214507543, 0.7040055610898279, 0.7040007565976721, 0.7040166465388168, 0.7039467086029102, 0.7039526634410503, 0.7039110205384542, 0.7039172006625064, 0.7039042997102726, 0.7039105438904398, 0.7039119897865532, 0.7039021786627168, 0.7038916534137986, 0.7039207446583724, 0.7039214349671877, 0.7039322286183096, 0.7039422538737373, 0.7039353804251146, 0.7039391496917502, 0.7039280119186927, 0.7039257442058443, 0.703921932009748, 0.7039056245857193, 0.7039003066610237, 0.7038929498405717, 0.7038802489113611, 0.703827544387003, 0.703813459934515, 0.7037936884138513, 0.7037834159422905, 0.7037160867331149, 0.7037133231850753, 0.703717489214411, 0.7037077055776217, 0.7037146720607352, 0.703696766367914, 0.7036964976150512, 0.7037060127869031, 0.7036958706959328, 0.7036581018251393, 0.703640697468019, 0.7036682212896904, 0.7036590194505603, 0.7036550166218659, 0.7036590096727732, 0.7036496223768548, 0.7036656819098955, 0.703666014876068, 0.7036395689601022, 0.7036234182413366, 0.7036252529931938, 0.70362629791913, 0.7036324308282961, 0.7036419676042985, 0.7036661975867836, 0.7036628661510687, 0.7036804298936904, 0.7037043330863053, 0.7037188820125254, 0.703721089882937, 0.7036868817125225, 0.703700634244398, 0.7036905662293228, 0.7036550229702168, 0.7036453361957489, 0.7036627616130401, 0.7036526971718325, 0.703647402109853, 0.7036087127423153, 0.703585928285094, 0.7035772134511499, 0.7035544642860438, 0.7035126998199553, 0.703510880495802, 0.7035024548226786, 0.7034924257916261, 0.7034854457403187, 0.703485609484651, 0.703461196714079, 0.7034854241904652, 0.7034851298818846, 0.7034842069322568, 0.7034677391344202, 0.7034213123608484, 0.70343898283018, 0.703426683295067, 0.7034097311236825, 0.7034177825098206, 0.7034319406438474, 0.7034232443238836, 0.7034393189658408, 0.7034475945828849, 0.7034673510565727, 0.7034716318054542, 0.7034678396401932, 0.7034649831520517, 0.7034859812410214, 0.7034788769735281, 0.7034712742335816, 0.7034640322823709, 0.7034667473402295, 0.7034357478850719, 0.7034243385683722, 0.7033982790456768, 0.7033695383436408, 0.7033488827473361, 0.7033322713971627, 0.7033445754635158, 0.7033322250708581, 0.7033456843808864, 0.7033490309130431, 0.7033367254183833, 0.7033108632888759, 0.7033144458846476, 0.7032766500107934, 0.7032718616055492, 0.7032500737280101, 0.7032519359927288, 0.7032212784181867, 0.7032007452161976, 0.7031898634189014, 0.7031777001560723, 0.7031393001465694, 0.7031485974603827, 0.7031459978346372, 0.7031601381121607, 0.7032236545613585, 0.7032510231654182, 0.7032060815733199, 0.7032170182831301, 0.7032042861038664, 0.7032283078975782, 0.7032154673592336, 0.7032202208879187, 0.7032046033443864, 0.7031862966606931, 0.7032250024279451, 0.7032458600815721, 0.7032808732727766, 0.7032794256338953, 0.7032953617251857, 0.7032982123201915, 0.7032908981055096, 0.7032767417147555, 0.7032583489708157, 0.7032263532560618, 0.7032148688636953, 0.7032274273580205, 0.7032344988924243, 0.7032177123256034, 0.7032006066679103, 0.703230063625807, 0.7032532024861337, 0.7032586778860569, 0.7032036639238742, 0.7031842541889057, 0.7031871394458064, 0.7031691223799518, 0.7031807379931778, 0.7031465626182979, 0.7031595105447229, 0.7031500341351631, 0.7031528443778327, 0.7031369504407025, 0.703122554596412, 0.7031220581116039, 0.7031243681504451, 0.7031196800932745, 0.7031452721624646, 0.7031369088167492, 0.7031348149553355, 0.7031485818167886, 0.7031468168267745, 0.703149477040982, 0.7031597824062156, 0.7031731488721169, 0.7031460392556076, 0.7031366637629781, 0.7031628317599102, 0.7031699808492556, 0.7031503858228415, 0.703160630998945, 0.7031608937099142, 0.7031291371552184, 0.7031375824238637, 0.7031243909603062, 0.7031234062795999, 0.7031423357647086, 0.7031490724055768, 0.703142136943863, 0.7031232721724182, 0.7031212018288179, 0.7031287476934102, 0.7031545557849963, 0.703149124777236, 0.7031675209216246, 0.7031616561214008, 0.703168661168727, 0.7031416156322726, 0.7031432268963643, 0.703159056674482, 0.7031364320080792, 0.7031348286337818, 0.7031313725064038, 0.7031315803200014, 0.7031263807396275, 0.7031134075437518, 0.7031254233023888, 0.7031335377848179, 0.703131054711737, 0.7031134442775858, 0.7030821621240323, 0.7030526759239905, 0.7030564742977857, 0.7030565894145724, 0.703035266907567, 0.7030144988881982, 0.7029971437120492, 0.7030068244217654, 0.7029971590811298, 0.7029844609942556, 0.70297224693164, 0.7029242908601706, 0.7029324810144518, 0.7029215061114947, 0.7029204103554871, 0.702899131836698, 0.702864932544529, 0.7028592077249373, 0.7028287783844253, 0.7028212418595667, 0.7028365483440009, 0.7028417012722882, 0.7028223208399633, 0.7028138699323615, 0.7027966250665246, 0.7028062006928626, 0.7027967977082107, 0.7028066024366099, 0.7027966121504717, 0.7028368533311364, 0.702820983031951, 0.702828380943914, 0.7028306514259096, 0.7028413596346031, 0.702834693323137, 0.7028425196362246, 0.7028361123340131, 0.7028634278275989, 0.7029138079047245, 0.7029105257736453, 0.7029324853013915, 0.7029699347827149, 0.7029569594753852, 0.7029946639695451, 0.7029877776627719, 0.7029720221689405, 0.702966749355609, 0.7029678641439062, 0.702971174256546, 0.7029796759378314, 0.7029921569396601, 0.7029725790877559, 0.7029754827103837, 0.7029980332630349, 0.7029819212376266, 0.7029691035247184, 0.7029778285268402, 0.7029697453036218, 0.7029740319958261, 0.7029613855633571, 0.7029385600458475, 0.7029372816339262, 0.7029131171874919, 0.7029001359138373, 0.7028904743137822, 0.7028760203119205, 0.702864519755776, 0.7028575395275166, 0.7028602643253782, 0.7028363901565667, 0.7028186925054917, 0.7028249560688239, 0.7028085364236327, 0.7028159169625733, 0.7028103634826933, 0.7028128792994633, 0.7028206314148658, 0.7028112035078893, 0.7028208549623365, 0.7027996059199704, 0.7028182686281079, 0.7028036334266169, 0.7028061878294481, 0.7028036019497859, 0.7028271157464266, 0.7028328734875248, 0.702819009157109, 0.7028232197806532, 0.7028249125432499, 0.7028459338768172, 0.7028583272161694, 0.7028925394865485, 0.7028894044119456, 0.7028858921739861, 0.7028731018664991, 0.7028965362272004, 0.7028994626491616, 0.7029065725686628, 0.7029201136346197, 0.7029369649077495, 0.7029363420748953, 0.702922639244752, 0.702910411325368, 0.7028846905642029, 0.7028680284208771, 0.7028658273923206, 0.7028637896509343, 0.7028739390411384, 0.7028791774342796, 0.7028961159657952, 0.702909476505896, 0.7028965535683508, 0.7028873513655102, 0.7028993611470061, 0.7028859191251295, 0.7028877965951006, 0.7028697252305115, 0.7028858988615311, 0.702867231851095, 0.7028669536820645, 0.7028822346001813, 0.7028816311242448, 0.7028668033056111, 0.7028634387699234, 0.702849270828978, 0.7028617047320136, 0.7028866042994529, 0.702897683694362, 0.7029033790325553, 0.7029171887106932, 0.7029035226412801, 0.7028985529255771, 0.7029164000111643, 0.7029307763162489, 0.702951612529554, 0.7029495168429533, 0.7029433090463213, 0.7029311449634847, 0.7029133517262479, 0.7029116976284541, 0.7029156186402167, 0.7029050443561285, 0.7029182820337191, 0.7029005882637621, 0.7029002012245521, 0.702886644004263, 0.7028736662906315, 0.7028598992838816, 0.7028735513766113, 0.7028573442307843, 0.7028524078319536, 0.7028603003702241, 0.7028868888285879, 0.702901335153156, 0.7029111272413858, 0.7029073987670148, 0.7029155212365117, 0.7029127971109089, 0.7029241753206894, 0.7029122840512375, 0.7029350317355423, 0.702948932918409, 0.702948269490288, 0.7029570045773753, 0.7029643696072027, 0.7029481347934105, 0.7029484654093808, 0.7029446263320133, 0.7029549354988301, 0.7029216825091201, 0.7029118946763188, 0.7028829515818767, 0.7028941188554408, 0.7028798670747299, 0.702870968485808, 0.7028715561138695, 0.702856761774278, 0.7028426673405873, 0.7028284058117974, 0.7028226225250768, 0.7028392541038763, 0.7028660043473375, 0.7028638055520248, 0.7028647960048703, 0.7028655416529322, 0.7028329324074893, 0.702795505398852, 0.7027994883378978, 0.702820206512401, 0.7028028336603779, 0.7028144760123908, 0.702793055927252, 0.7027751227897111, 0.7028094424105303, 0.7028015950526311, 0.7028095536862908, 0.7028147512369086, 0.702819075271679, 0.7028302387347183, 0.7028037532360925, 0.7027863902848722, 0.7027758360629862, 0.7027659779479413, 0.7027867441783934, 0.7027837726655465, 0.7027781645153618, 0.7027406715644197, 0.7027635401678637, 0.702792787083173, 0.7027866072895178, 0.7027686255151292, 0.7027654521091491, 0.7027621872231733, 0.7027725335587051, 0.7027784081115506, 0.7027513175628077, 0.702749930066403, 0.7027559797820316, 0.702752292526081, 0.7027630975658012, 0.7027801923364043, 0.7027668981317391, 0.7027448923502285, 0.7027237581604471, 0.7027142023543903, 0.7026894778904618, 0.7026832783006092, 0.7026948271739485, 0.7027161231087358, 0.7027079253991669, 0.7026869626529321, 0.7026900314148131, 0.7026954548630316, 0.7027093034870475, 0.7026624939334436, 0.7026626693415366, 0.7026691800700221, 0.7026579217439158, 0.7026572171828221, 0.7026372489699281, 0.702635179054575, 0.7026081375782095, 0.7026046314130778, 0.7026106674654026, 0.7026503430645364, 0.7026375764700581, 0.7026277457305098, 0.7026163137788096, 0.7026269068409361, 0.7026474087885459, 0.702633377408603, 0.7026003754948723, 0.7026020163824863, 0.7026051901399907, 0.7026529041933459, 0.7026486351491098, 0.702640558051646, 0.7026632101420739, 0.7026620875650771, 0.702650365906846, 0.7026522537830218, 0.7026575094446291, 0.7026448988091006, 0.7026277366290197, 0.702613689833703, 0.7026168713326045, 0.702602410068789, 0.7026475748697406, 0.7026702865364561, 0.7026855676145076, 0.7026757193308257, 0.7026763220832347, 0.7026608779892817, 0.7027011008209483, 0.7027042587964841, 0.7027014042258996, 0.7026987954066545, 0.7027059427202064, 0.7027031371049683, 0.7026865332984013, 0.7026850000293954, 0.7027127032414447, 0.7027000534579927, 0.7027141883050225, 0.7027223667697284, 0.7027241144385658, 0.7027205499573705, 0.7026952084203245, 0.7027011647533268, 0.702710407817439, 0.7027033541076669, 0.702698380605379, 0.7026998068414341, 0.7026982793892743, 0.7027129525915293, 0.7026966362839917, 0.702696731458419, 0.7027208540156161, 0.7027344610618066, 0.7027349981857949, 0.7027301665162221, 0.7027215831682051, 0.7027109996228366, 0.7027091043902459, 0.7027163235796913, 0.7026949462753996, 0.7026964233423918, 0.7026913440766069, 0.7026954362331024, 0.7026995399405823, 0.7027115633326712, 0.7027217285746438, 0.7027039225361493, 0.7026918175203769, 0.7026886614233154, 0.7026938901049364, 0.7026966999154652, 0.702707365473754, 0.7027156937404323, 0.7027153884988291, 0.7027092396386023, 0.7027025304451943, 0.7027103313715841, 0.7027081071005208, 0.702714343519894, 0.7027381784504311, 0.7027409281692473, 0.7027551410460471, 0.7027585116812425, 0.702763203429167, 0.7027436809398003, 0.7027264973144941, 0.7027391628883514, 0.7027316168705305, 0.7027085977047398, 0.7027147154059858, 0.7026977498889315, 0.7027054690815296, 0.702710639565704, 0.7026959601350851, 0.7027336784724573, 0.7027261987517107, 0.7027242084625168, 0.7027131308863049, 0.7027280890522873, 0.702739801515865, 0.7027271122579865, 0.7027244465376447, 0.7027193294103355, 0.7026965261111888, 0.702695401591758, 0.7027040702684179, 0.7026965025819102, 0.7026644660280938, 0.7026565153252577, 0.702659626352994, 0.7026675462361873, 0.7026681437376504, 0.7026548111525749, 0.7026509588457026, 0.7026579986997877, 0.7026631531821375, 0.7026690977277866], 'valid auc-stdv': [0.004774375175480904, 0.002783245156423366, 0.003299038161212286, 0.0044357215242080816, 0.004724409432618025, 0.004282360214839741, 0.004883387337602143, 0.004825220875033634, 0.004587183323031289, 0.004176894396025091, 0.0036821807794487267, 0.003453214450122156, 0.0035611985524901233, 0.0036885786683666464, 0.0037623927123634, 0.004033197779065605, 0.003945598954602912, 0.004153057067242886, 0.004276419232447204, 0.004274384439184623, 0.004194198149302373, 0.004228144661455911, 0.004147753633966857, 0.00415207966611278, 0.004023448064931474, 0.003924723623434714, 0.003940599092297846, 0.0038725567667347626, 0.003836894162932987, 0.0036514958285359875, 0.0036045166231149445, 0.0036713954599956237, 0.003669242863465994, 0.0036816362546769556, 0.0036733369216121097, 0.003607451314972379, 0.003546677679564051, 0.00354425404003073, 0.003662333653821967, 0.0036412011122249144, 0.003530813234173178, 0.003598681635758015, 0.0037027747039056124, 0.0036789740957022697, 0.0036796027302710716, 0.0036357903938699994, 0.0036315768770295315, 0.003556270787395176, 0.0035663394296294023, 0.0035946968517572685, 0.0035837194916041536, 0.003547954352535272, 0.0034860291979536048, 0.0034584568200630903, 0.0034239444004363086, 0.0033297390218024535, 0.0032411536773394953, 0.0032697715024960646, 0.0032522715828557864, 0.0032282314716131208, 0.0031901165592428823, 0.003135624957918422, 0.0031808830809440334, 0.0031556918054020406, 0.003090895292400015, 0.0031307463565525655, 0.00322884288686197, 0.0032468816473959702, 0.0031379758861714915, 0.003121459942982248, 0.0032286014098649985, 0.00321870839604377, 0.003311974859055034, 0.0033389433888478793, 0.0033246987842196814, 0.00338156514913133, 0.0033337597161282886, 0.0033798582494570803, 0.003382139099536414, 0.003438327792965369, 0.0034436663015885017, 0.003427630069359877, 0.0034295671465067773, 0.0034427814085054554, 0.0034227945809204396, 0.003431208825001776, 0.0034927184393461403, 0.0035058064847480843, 0.003459605520070757, 0.0034622605049441386, 0.0034484047932995315, 0.003464326680043765, 0.0034214340021974155, 0.0033873283465126426, 0.0033807440071212387, 0.0033828690395799315, 0.003337966784855654, 0.003372707260025915, 0.0034056085143162503, 0.0034252461132364236, 0.0034907096415698695, 0.003460929791828124, 0.003511538869686153, 0.0035103261428393352, 0.0035136833186667966, 0.0034958963841361136, 0.003482963482995303, 0.003421321387644256, 0.003429909086537037, 0.0034057433923201322, 0.0034153372748565634, 0.003312356148843968, 0.0033259864494473755, 0.0032679805951265147, 0.0032648308996545165, 0.003225726979829308, 0.0032188570671873406, 0.003231456647579317, 0.003216937226437685, 0.003226805138773042, 0.003177543142267011, 0.0031629733047219637, 0.003186784969115842, 0.003170899072682473, 0.0030991964171349093, 0.003122587145800234, 0.0030630788504952796, 0.0030527002938546487, 0.003060317627420052, 0.0030328520528381205, 0.0030524385805056633, 0.0030215182938435657, 0.0030093044578897287, 0.0030889933686101914, 0.0031083811579505276, 0.0030942688945795145, 0.0030438074733243766, 0.0030391498172080964, 0.003040077180112756, 0.0030211069756975206, 0.002995163872301903, 0.0030181697053964724, 0.0030645306673279327, 0.00305193448226254, 0.0030377502687440527, 0.003014583195791184, 0.0030286890062069445, 0.0029645225110748502, 0.002981540795052557, 0.002989379273718617, 0.0030074982746617403, 0.0030165986022891384, 0.0030354322433572252, 0.0030213102575941545, 0.0030058167714187294, 0.003075066077150976, 0.0030584876381176134, 0.0030081991883997767, 0.0029848630894423493, 0.0030063827551403176, 0.0030251547656338722, 0.0030588019477871637, 0.0030739623599943594, 0.0030963178364188703, 0.003066066673792085, 0.0030627544667892245, 0.003099834214115366, 0.0030384269590592846, 0.0030421078219502657, 0.00307557443803197, 0.003054525261868037, 0.0030691332717661755, 0.0031167933775529126, 0.0031377711630914146, 0.003079038712045323, 0.0031028601417153762, 0.0030567211004815732, 0.0031279286215805716, 0.0031417036896433633, 0.003203490304204735, 0.003180186458184566, 0.00321002219905495, 0.0032168605550094336, 0.00318132703194868, 0.003176156085111945, 0.003180421249503205, 0.0032453140626843204, 0.0032310952710719078, 0.0032417123173979624, 0.0032667729602599387, 0.003210576553007615, 0.003160637863452871, 0.0031044673550082403, 0.0030961978498332135, 0.003088302562784789, 0.0030974293126674324, 0.0030694625417459188, 0.0030743902177476925, 0.0031289676343451488, 0.003114910187188191, 0.003131226589061381, 0.0031342833430417806, 0.0031142950339447363, 0.0031375375542337276, 0.0031021500253835604, 0.0031460737129866425, 0.0031867541293388595, 0.00317722259554835, 0.003157901673090031, 0.003151028660304196, 0.0031691969474620134, 0.0032206378756309204, 0.0032587618468108296, 0.0032653215310453746, 0.0032968519644213587, 0.003310581062124739, 0.0033123440908239335, 0.0033480250488986847, 0.0033736002414689833, 0.0033300857662806587, 0.003306070242279139, 0.003362537416145068, 0.0034199912121255305, 0.0034045604878622615, 0.0034314998795476077, 0.003469414126517301, 0.003433130790175375, 0.003419612723862991, 0.0034827800825869035, 0.003456587125222275, 0.0035058413005634305, 0.0035021946619764825, 0.003462255610730989, 0.003474204660073145, 0.0034018510657657167, 0.003444730456570126, 0.0034069218891133743, 0.003417930918069448, 0.003400114478600696, 0.003392805163186172, 0.0034040550206379206, 0.0034227274232836893, 0.0033879256863694513, 0.0033593062505384514, 0.00338493540529267, 0.003381640199816792, 0.0034229240058777253, 0.0034142861635660065, 0.0034009009461003136, 0.0033780733996276596, 0.0034125685716860665, 0.0034173518358466934, 0.003370547481380284, 0.00333905181646241, 0.003347431973392543, 0.003337305300729499, 0.003312307541968293, 0.0033456632728249714, 0.0033404284849985013, 0.003309290572745608, 0.0032681614234193325, 0.0033120553655591584, 0.0032431310350538646, 0.0032223474370653396, 0.003227327465813927, 0.0032866410267183995, 0.0033183250182997255, 0.003308316311029285, 0.0033150942133669263, 0.0033234136375769937, 0.0033201099895987934, 0.0033330601796154346, 0.0032769967614222043, 0.0032908059548179954, 0.003291329892322996, 0.003245146696720622, 0.00325103365670707, 0.003195735710882155, 0.0031834736112421086, 0.003165440772386436, 0.0031719362287571046, 0.0031718951729416568, 0.0031443531576283933, 0.00315395420433192, 0.003128211053411125, 0.0031372174898202662, 0.0030943946572906116, 0.0030879102512436393, 0.003057124613492067, 0.0030378967815623634, 0.0030694907861357184, 0.0030775957508944497, 0.003011284252220946, 0.002964310747082798, 0.0029276950697966243, 0.0028866874998106176, 0.00284419184369489, 0.0028546343716477633, 0.002842304883743206, 0.00287019991329135, 0.002875890196181571, 0.002863947525440201, 0.0028647806683455403, 0.002855824550106105, 0.0028547326760690155, 0.0028587076532343085, 0.0028762351823520883, 0.002838292973232389, 0.002836483357575387, 0.002833325336227582, 0.002825624076223402, 0.0028468396880931396, 0.0028161935919848165, 0.0027890570042004435, 0.0027560620256519945, 0.0027352818082846766, 0.0027238208634537526, 0.002741049200903618, 0.0027495252867199794, 0.002769247534768511, 0.0027259081176899793, 0.0027229123875997293, 0.002735238977793321, 0.00274652854228348, 0.002780021528364121, 0.002755638401733361, 0.002745718002268699, 0.0027571475910326514, 0.00276810989928684, 0.002764364344915059, 0.0027637476772793762, 0.002793313410606673, 0.002772973787625594, 0.0028136770285277857, 0.002839944644044947, 0.0028241512454668656, 0.002893979088131607, 0.0029018655462335514, 0.002942734057680228, 0.0029286402159212337, 0.0029407225851606354, 0.002952890029975264, 0.0029522172638931977, 0.0029804103960457294, 0.0029346063043200234, 0.002956552851378211, 0.0029440881583570446, 0.0029197656300729486, 0.002923985956856377, 0.0029400031842686187, 0.0029469993536501192, 0.002989724569757493, 0.0029416601459519736, 0.002906863197419813, 0.00291646603811838, 0.002905577864731846, 0.0028715348059653793, 0.0028665655220693094, 0.002873718513498536, 0.0028906595182500777, 0.0029080641394109512, 0.0029196870494004926, 0.002899181860833629, 0.002896599536097218, 0.002890370444113596, 0.0028907041422927527, 0.0029176641768162443, 0.0029117983690329988, 0.002909421157358955, 0.002869976481443, 0.002873728396729607, 0.00280742642705513, 0.0027924880698272122, 0.0027804117874698745, 0.00281359441916629, 0.002833203591022451, 0.0028388814873629414, 0.002820174212377973, 0.0028644566687303424, 0.002892677313471889, 0.0028727956076802993, 0.0028880713340162516, 0.0028407940916339524, 0.0028553062594771175, 0.002892803023514734, 0.002944110844931015, 0.0029365095719572796, 0.0029383735306123876, 0.002931240674372577, 0.002932435258275075, 0.0029456615543257026, 0.0029771112050874416, 0.002965270109384948, 0.0029715900208174563, 0.0029750573828080866, 0.0029458187904899793, 0.002944553827920727, 0.002925328642332507, 0.002935298722204059, 0.0029760032014510165, 0.0029783453613676744, 0.0029557693460045636, 0.0029462759389921912, 0.0029261459631484367, 0.0029086900323209283, 0.002913235895490996, 0.002905001208320473, 0.0028869068906504807, 0.002936266621199562, 0.0029354542716145513, 0.002965335483665716, 0.0029781843132439902, 0.002936021036594371, 0.002925387888215154, 0.002882986272931211, 0.0029278662968312713, 0.002918462197877838, 0.0029095422075827307, 0.0028947324097804354, 0.0029151127544624453, 0.0029409243498402745, 0.002957830399089011, 0.002949401285559643, 0.0029014882137624, 0.0029067422887370745, 0.0028871954343349018, 0.0029056818499991386, 0.0028914031758183883, 0.0029025260621272295, 0.0029054600558317623, 0.0028723312248889275, 0.0028441283541511845, 0.0028043422851389793, 0.0028021417327259964, 0.002792219393679844, 0.0028002200507112967, 0.002793547069530692, 0.002798554635761037, 0.0027967059627984655, 0.0027728253716131136, 0.002798831172514114, 0.002808448598802797, 0.0028014998546330647, 0.0028389390748718183, 0.0028310132176623103, 0.002810006846363776, 0.002854209281541349, 0.0028687894581796414, 0.002918466054813749, 0.0029089109737769434, 0.0028805737970870157, 0.0028936375650943616, 0.0029250032693233716, 0.002936649722610234, 0.0029102927806253483, 0.0029256381270024775, 0.0029263325113965764, 0.0029512119469533972, 0.002955727295526709, 0.0029426113160366915, 0.0029282724571423717, 0.0029172204350805136, 0.002929225555432975, 0.0029388042881558813, 0.0029553234135261588, 0.0029268779974755956, 0.002952516928158313, 0.0029817192017465364, 0.0029706526519456965, 0.0029829178065469085, 0.0029761540834409195, 0.002999634463847402, 0.0029893028718477196, 0.002976341708529871, 0.0029520685138241876, 0.002953919607834339, 0.0029660133229940013, 0.0029676462796039468, 0.0029953452263079814, 0.0029898013064024852, 0.002991562720120934, 0.0029991116461268808, 0.002982391892820427, 0.0029807843187049515, 0.003000764082079712, 0.003036214011878162, 0.0030223642670732876, 0.0030743929895068727, 0.0030743027388664738, 0.003073912849373024, 0.003084849288140674, 0.003079343726287136, 0.003100295433831761, 0.003120811099601198, 0.0031001049220234634, 0.0031092150135629, 0.0030682003686049344, 0.0030654194902932723, 0.0030813948390822628, 0.0030765714739960973, 0.0030785734351352423, 0.0030806314422392443, 0.0030914134584382733, 0.0030757019853567104, 0.003068780065142837, 0.0030991164510171683, 0.0031270104278489147, 0.003159951564017422, 0.003153891108413708, 0.003164582380137666, 0.003163539882282151, 0.0031461421086010897, 0.00313089033043321, 0.0031663396528187225, 0.003168175882742508, 0.003179290991020582, 0.003190342298862488, 0.0032011020875074192, 0.0031979375832647604, 0.0032071659324541986, 0.003190434611656298, 0.0031876051882790304, 0.0031792764823613366, 0.0031724393463553, 0.003184935979354097, 0.0031609442407238447, 0.003134674566092827, 0.0031228727884477303, 0.0031560290237907782, 0.0031823552330656935, 0.0031864439961040987, 0.003196306788526014, 0.003162354423954197, 0.0031438332633846175, 0.0031397122099852772, 0.003155720521434739, 0.0031771622427341443, 0.0031665612340386795, 0.0031428210675394563, 0.003128813128790096, 0.003088568535087949, 0.0030653036613321424, 0.0030600005160102473, 0.0030572213639800173, 0.0030805103569916136, 0.0031035709583402396, 0.0031312174295298626, 0.003109660304048696, 0.003113600888075087, 0.0031160051152086857, 0.003113134758593048, 0.0031329580324461104, 0.003167849053118623, 0.003174171212777594, 0.0031593085887743106, 0.003110188793214283, 0.0031255197104914736, 0.0031244236919449166, 0.003108866238953312, 0.0031218518158497954, 0.003119315988867038, 0.0031169347986907524, 0.003093081899631611, 0.0031008497559594527, 0.0031472557570135554, 0.0031421798398121557, 0.003176595012509067, 0.0031798720901472046, 0.0031647700007368896, 0.0031754694956786576, 0.003196822793743381, 0.003180281449163265, 0.003172904545188311, 0.0031828835710202654, 0.0031701764824268604, 0.003147945792470837, 0.003119284674425153, 0.003104917331193287, 0.0030794893588224482, 0.0030940711827158475, 0.003108022826506502, 0.003090901801398945, 0.003120355697863535, 0.0031380385337014564, 0.0031039333810115796, 0.0031053244401094333, 0.003099083503636394, 0.003102350842609585, 0.003076582905903587, 0.0030816986555533285, 0.0030923768244024174, 0.0031083892588418273, 0.0030930859607772353, 0.0031172667423224006, 0.003118212426862359, 0.003117416076545407, 0.0031233825762987245, 0.0031248680210566415, 0.0031295806602419517, 0.003118224133321531, 0.0031175101843124694, 0.00309884421672459, 0.0030870308724348944, 0.0030998077702718086, 0.003102882518250383, 0.0031058853581086517, 0.003112117432393911, 0.0031026548240209273, 0.0030999359509016754, 0.003095105863186429, 0.003084065510578044, 0.0031054020156511828, 0.0031104144424803654, 0.003107086810830684, 0.003104629665265396, 0.0031272221427705517, 0.0031010669163737343, 0.003097648440265759, 0.003110614345527702, 0.0031089386831821817, 0.003113817400225458, 0.003119742781188038, 0.003115877592737703, 0.003118577216921875, 0.0031116682208931026, 0.0031075515125139567, 0.003101866137668106, 0.003102006807787394, 0.003100898396244545, 0.003117353529486623, 0.003139874810524256, 0.003151709352822729, 0.0031573034778926558, 0.003158291919637426, 0.0031479370768195385, 0.0031563486428955716, 0.0031719469536442627, 0.0031895416264808717, 0.0032129483316828663, 0.003216345390555999, 0.0032020691621559047, 0.003180417304583373, 0.003166250941478485, 0.0031650089464659, 0.003163949363078907, 0.0031741714450093344, 0.0031385011020889403, 0.0031300570002979378, 0.003131709767904026, 0.0031331174466087543, 0.0031440511892171156, 0.0031567586005066374, 0.0031828884502697343, 0.003213990325794014, 0.003220425474091104, 0.003209711801679182, 0.003217553991701023, 0.0032034141755963655, 0.0031904649087731163, 0.0031637183730474144, 0.0031645447454581766, 0.0031919345963832097, 0.0031915579454280265, 0.0032220997927613843, 0.0032523700062433163, 0.003260962617242296, 0.0032306869722091383, 0.0032202349280206497, 0.0032370747360773704, 0.0032583216441851954, 0.003252294314789395, 0.0032398056806018467, 0.0032403006252120165, 0.0032139438375815344, 0.0032502277750232275, 0.0032791359866578434, 0.003247758099749262, 0.003225954469595663, 0.0032694340590037506, 0.003280389480151073, 0.0033170699505584585, 0.0033080184186439123, 0.0032949819861056496, 0.00326441813447584, 0.0032720944456657987, 0.003259021377812204, 0.0032481169981399055, 0.0032364912033585568, 0.0032589017143514615, 0.003237485610022421, 0.003245446680857502, 0.0032557742577157415, 0.0032719478738645817, 0.0032876771623966206, 0.003291813079316541, 0.0032958489561769138, 0.00330977026320501, 0.0033349255222558994, 0.003333820844212209, 0.0033672222094897858, 0.003388273361343229, 0.0034090589705323133, 0.003413537189885084, 0.003438951376061814, 0.003460563514559343, 0.0034262814391760603, 0.003441723299879715, 0.003439679661711653, 0.0034515874516379873, 0.003472651525512972, 0.003481255970580239, 0.0034781908491144276, 0.00349001076850428, 0.003503515272066235, 0.0035129859737438557, 0.0035164474732539315, 0.0035235311414344166, 0.0035315523996721824, 0.0035490243964612605, 0.0035790686146512786, 0.0035682534222376966, 0.0035751408997957562, 0.0035808379596800376, 0.0035806034235341755, 0.0035943946514512158, 0.003602977959573399, 0.003596249827126542, 0.003591631427361376, 0.003594780047371572, 0.003610535374398526, 0.00360697036406121, 0.003610787169738579, 0.0035975783766873308, 0.0036076437880306167, 0.0036240125066871226, 0.0036387159188929156, 0.003636574451452641, 0.003644791482715562, 0.0036448315336922482, 0.0036480124844591962, 0.003661683359740325, 0.003641541395471304, 0.0036561232064969964, 0.003680272998416214, 0.0036957500398980122, 0.003689751487363129, 0.0037057319198475587, 0.0036944906621454027, 0.003717458825539843, 0.0037276543954256228, 0.0037366217788762602, 0.003733262355710484, 0.0037503356022207924, 0.003754110416316594, 0.0037805919050198336, 0.0037959090621707296, 0.0037915987454819355, 0.003793450277527966, 0.0038079047905749345, 0.003823690999695946, 0.0038302229551725256, 0.0038328877055659257, 0.003845205772944512, 0.003834869693076205, 0.0038569335919957613, 0.0038692397852005656, 0.0038783014331720254, 0.00389005021117684, 0.0038876479700956364, 0.003914077488421091, 0.0039018830133785523, 0.003911926031637008, 0.003911026729145635, 0.003937240168399371, 0.003963662961755993, 0.0039737569224013594, 0.0039805983541757305, 0.003976689601136753, 0.004000292853384227, 0.0040048146379405855, 0.0040028089365530236, 0.003992273795995404, 0.004011394812470383, 0.004052915204098005, 0.004051482000702873, 0.00405877777264379, 0.004058228179885379, 0.004042740090610317, 0.004036898193347308, 0.004034318542666463, 0.004025015586070289, 0.003999672321985149, 0.004010917009722081, 0.003987809429759201, 0.00398217849754653, 0.0039403984948098115, 0.003945191835134164, 0.00392146544318019, 0.003927299540801455, 0.003918813183650323, 0.0039123410361854676, 0.003917021615048515, 0.0039398424860348615, 0.003948886683120233, 0.003942102745798255, 0.003945385328351894, 0.0039443735156622, 0.003951276048104705, 0.0039624876214979085, 0.003967353290380168, 0.003979143153680499, 0.003985667226412311, 0.00398717971901772, 0.003980650433797341, 0.004008266620857889, 0.004021675760859468, 0.004030474460935833, 0.004018829381648142, 0.003997553746744932, 0.003995782577788933, 0.003995164434924811, 0.0039728582157865025, 0.003991851781296077, 0.003998919885930607, 0.004013910636263389, 0.003987603902917134, 0.003993941888202033, 0.003981572904782166, 0.003977617405437268, 0.003977945475689093, 0.0039627939577319225, 0.003943369615821632, 0.003943029490542515, 0.003936668665983383, 0.003917939464978101, 0.003921185474808117, 0.003935513839479021, 0.00393258999831388, 0.003923051114001473, 0.003942297568096754, 0.003948479311711536, 0.0039580295096056225, 0.003965871028730117, 0.0039597907493579085, 0.003920236242443032, 0.003897614015698843, 0.003882515429727213, 0.0038806928639075057, 0.0038646233317565695, 0.0038495473744068064, 0.0038417693537484685, 0.003843739876374966, 0.003849808803627419, 0.0038547680722947216, 0.003854904830930211, 0.0038449349366207444, 0.0038655265519087346, 0.003849595001293769, 0.003851880677687354, 0.003849987770189582, 0.003854752816364861, 0.003834094462299784, 0.0038447609120569972, 0.0038463364626751106, 0.0038383695086000457, 0.0038197433836335316, 0.0038055045557589414, 0.003801712499037975, 0.0037952589441357315, 0.0037782010306791312, 0.003787566248100512, 0.0037710507690557646, 0.0037529016196947854, 0.0037608906733931843, 0.003734088167256358, 0.003744743416328387, 0.003724765304025239, 0.0036966600703177933, 0.003677931138800304, 0.003661763254001068, 0.003663473947143651, 0.003703422980639486, 0.0036906094565017033, 0.003703311081130137, 0.003702471421618011, 0.003715240188346986, 0.0037018614366710317, 0.003684033964542488, 0.0036797029026299855, 0.0036518043725934104, 0.0036492166047210335, 0.003687218235555753, 0.0036764448619520385, 0.0036659945071203137, 0.003654789227736062, 0.00364538707802287, 0.0036301097531846543, 0.003607619646002912, 0.0036149235127444623, 0.00358250245950686, 0.0035591515525921274, 0.003533044584473894, 0.0035310225538481106, 0.003502169138285422, 0.0035473801175336765, 0.0035798758659062765, 0.0035726856239091474, 0.003583895267664111, 0.0036099617975452226, 0.003593463356608152, 0.0036102648677589144, 0.0036198849803966596, 0.003619347250326754, 0.003586730128282414, 0.003579278424809188, 0.003561917568024694, 0.003546050085260608, 0.003542540297029062, 0.0035501310651055457, 0.0035315390208709575, 0.0035276563975165064, 0.0035307194973735172, 0.0035124238073416403, 0.003492959970217146, 0.003518185115148166, 0.003517308133241187, 0.003507742412722926, 0.00350444881416451, 0.0034973686111225213, 0.003504060651267481, 0.003500311127662157, 0.0035349629502954964, 0.0035449254450064097, 0.003556340202410658, 0.0035788617911016365, 0.003604435658544011, 0.0036036548838083785, 0.0036074416517228127, 0.0036147128954857905, 0.0036122740039588506, 0.0036111979923040393, 0.0036099913349399207, 0.003606193873162573, 0.003599332828525675, 0.0036274882973027483, 0.003629036554472929, 0.0036360076669856305, 0.0036422571957772554, 0.0036409458567178057, 0.0036070153367516507, 0.003593822054443122, 0.003590954983250727, 0.0035811938134308724, 0.0035820649442598147, 0.0035681465569296455, 0.0035734533578995436, 0.0035573646636001175, 0.0035541144313581516, 0.0035444513039405495, 0.0035492751209628744, 0.0035564932622509393, 0.003562524265536317, 0.003564501444255891, 0.0035538728223401908, 0.003539507685051073, 0.003517307857603818, 0.0035009296984980054, 0.003477810127888105, 0.0034558237196143644, 0.003462190968086954, 0.003458616703376887, 0.0034333765431126557, 0.003439909620832773, 0.0034091605153430964, 0.003413972849148644, 0.0033940975032476055, 0.0033826648850459324, 0.0034063215940113595, 0.003395248051633834, 0.003375742245741991, 0.0033596852342909394, 0.003347834293019551, 0.003331304355022104, 0.0033441945469753925, 0.003338108599578965, 0.003310526195910603, 0.0032913610787107376, 0.003281455503989902, 0.003291713450500177, 0.0033051873780884813, 0.003289752966626445, 0.003280633223909644, 0.0033063204956605397, 0.0032956208307810923, 0.003288481963085922, 0.0032611089107796565, 0.0032323802997406958, 0.003252178892182269, 0.0032574322729604383, 0.003264578782283345]}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'dict' and 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m lgbBO \u001b[38;5;241m=\u001b[39m BayesianOptimization(lgb_eval, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_leaves\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m25\u001b[39m, \u001b[38;5;241m4000\u001b[39m),\n\u001b[0;32m      2\u001b[0m                                                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m63\u001b[39m),\n\u001b[0;32m      3\u001b[0m                                                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlambda_l2\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.05\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m                                                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_data_in_leaf\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m2000\u001b[39m)\n\u001b[0;32m      7\u001b[0m                                                 })\n\u001b[1;32m----> 9\u001b[0m lgbBO\u001b[38;5;241m.\u001b[39mmaximize(n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, init_points\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\bayes_opt\\bayesian_optimization.py:310\u001b[0m, in \u001b[0;36mBayesianOptimization.maximize\u001b[1;34m(self, init_points, n_iter, acquisition_function, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[0;32m    308\u001b[0m     x_probe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuggest(util)\n\u001b[0;32m    309\u001b[0m     iteration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobe(x_probe, lazy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bounds_transformer \u001b[38;5;129;01mand\u001b[39;00m iteration \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    313\u001b[0m     \u001b[38;5;66;03m# The bounds transformer should only modify the bounds after\u001b[39;00m\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;66;03m# the init_points points (only for the true iterations)\u001b[39;00m\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_bounds(\n\u001b[0;32m    316\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bounds_transformer\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_space))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\bayes_opt\\bayesian_optimization.py:209\u001b[0m, in \u001b[0;36mBayesianOptimization.probe\u001b[1;34m(self, params, lazy)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_space\u001b[38;5;241m.\u001b[39mprobe(params)\n\u001b[1;32m--> 209\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(Events\u001b[38;5;241m.\u001b[39mOPTIMIZATION_STEP)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\bayes_opt\\bayesian_optimization.py:62\u001b[0m, in \u001b[0;36mObservable.dispatch\u001b[1;34m(self, event)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdispatch\u001b[39m(\u001b[38;5;28mself\u001b[39m, event):\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_subscribers(event)\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 62\u001b[0m         callback(event, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\bayes_opt\\logger.py:122\u001b[0m, in \u001b[0;36mScreenLogger.update\u001b[1;34m(self, event, instance)\u001b[0m\n\u001b[0;32m    120\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_header(instance) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m event \u001b[38;5;241m==\u001b[39m Events\u001b[38;5;241m.\u001b[39mOPTIMIZATION_STEP:\n\u001b[1;32m--> 122\u001b[0m     is_new_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_new_max(instance)\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_new_max:\n\u001b[0;32m    124\u001b[0m         line \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\bayes_opt\\logger.py:116\u001b[0m, in \u001b[0;36mScreenLogger._is_new_max\u001b[1;34m(self, instance)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_previous_max \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_previous_max \u001b[38;5;241m=\u001b[39m instance\u001b[38;5;241m.\u001b[39mmax[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m instance\u001b[38;5;241m.\u001b[39mmax[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_previous_max\n",
      "\u001b[1;31mTypeError\u001b[0m: '>' not supported between instances of 'dict' and 'dict'"
     ]
    }
   ],
   "source": [
    "lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (25, 4000),\n",
    "                                                'max_depth': (5, 63),\n",
    "                                                'lambda_l2': (0.0, 0.05),\n",
    "                                                'lambda_l1': (0.0, 0.05),\n",
    "                                                'min_child_samples': (50, 10000),\n",
    "                                                'min_data_in_leaf': (100, 2000)\n",
    "                                                })\n",
    "\n",
    "lgbBO.maximize(n_iter=10, init_points=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rdkxhhST-KZS"
   },
   "source": [
    " **<font color='teal'> Print the best result by using the '.max' function.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:49:01.513767Z",
     "start_time": "2019-04-22T15:49:01.509392Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "oc8z6mfy-KZS"
   },
   "outputs": [],
   "source": [
    "lgbBO.max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:50:29.049881Z",
     "start_time": "2019-04-22T15:50:29.045908Z"
    },
    "colab_type": "text",
    "id": "J5LAydKC-KZW"
   },
   "source": [
    "Review the process at each step by using the '.res[0]' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:51:01.001688Z",
     "start_time": "2019-04-22T15:51:00.997484Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "X1ttZmrI-KZX"
   },
   "outputs": [],
   "source": [
    "lgbBO.res[0]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Bayesian_optimization_exercise.ipynb",
   "provenance": []
  },
  "deepnote_execution_queue": [],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
